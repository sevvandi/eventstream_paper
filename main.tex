\documentclass[a4paper,11pt]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{newtxtext,newtxmath}
\usepackage{amsfonts, mathrsfs, microtype}
\usepackage{amsmath, bm}
\usepackage[showonlyrefs]{mathtools}
\usepackage{paralist, parskip}
\usepackage{todonotes}
\usepackage{graphicx, subfig, rotating, booktabs, multirow, url}
\usepackage[no-weekday]{eukdate}
\usepackage[margin=2cm]{geometry}
\usepackage[linesnumbered,lined,boxed,commentsnumbered]{algorithm2e}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows}
\tikzstyle{standard} = [rectangle, rounded corners, minimum width=1.5cm, minimum height=2cm,text width=2.5 cm, text centered, draw=black, fill=black!5]
\tikzstyle{io} = [trapezium, trapezium left angle=70, trapezium right angle=110, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=blue!30]
\tikzstyle{process} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=black, fill=orange!30]
\tikzstyle{decision} = [diamond, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=green!30]
\tikzstyle{arrow} = [thick,->,>=stealth]

%% CAPTIONS
\RequirePackage{caption}
\DeclareCaptionStyle{italic}[justification=centering]
 {labelfont={bf},textfont={it},labelsep=colon}
\captionsetup[figure]{style=italic,format=hang,singlelinecheck=true}
\captionsetup[table]{style=italic,format=hang,singlelinecheck=true}

%% LINE AND PAGE BREAKING
\allowdisplaybreaks
\sloppy
\clubpenalty = 10000
\widowpenalty = 10000
\brokenpenalty = 10000

%% GRAPHICS
\RequirePackage{graphicx}
\setcounter{topnumber}{2}
\setcounter{bottomnumber}{2}
\setcounter{totalnumber}{4}
\renewcommand{\topfraction}{0.85}
\renewcommand{\bottomfraction}{0.85}
\renewcommand{\textfraction}{0.15}
\renewcommand{\floatpagefraction}{0.7}
%\RequirePackage[section]{placeins}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\E}{E}

% =======================================================================
\begin{document}
% =======================================================================

\title{Early classification of spatio-temporal events using time-varying models}
\author{Sevvandi Kandanaarachchi, Rob J Hyndman, Kate Smith-Miles}
\maketitle

\begin{abstract}
This paper investigates early event classification in spatio-temporal data streams. We propose a framework for early classification that considers the relationship between the features of an event and its age. The framework incorporates an event extraction algorithm as well as two early event classification algorithms, which use a series of logistic regression classifiers with penalty terms and state space models. We use this framework on synthetic and real applications and demonstrate its reliability and broad applicability. The algorithms are available in the R package \textit{eventstream}, and other code in the supplementary material.
\end{abstract}

%% Outline of paper
%% Introduction
%% 		- Small introduction to data-streams and interesting topics
%%		- Events that change over time
%%		- Real world example
%%		- Why standard classifiers may be not enough
%% 		- Example - RF and event classifier
%%		- General workflow
%%			 Data stream -> Pre-processing -> Event extraction -> Features -> Event classification
%%		- Main focus is on classification
%% 		- Two methods - static and updating
%%		- Mention eventstream package
%% Event extraction
%%		- threshold and cluster
%% Event classification
%%		- Cyclic logisitic regression (with penalty)
%%		- Cascaded DMA
%% Applications
%%		- Synthetic data
%% 			- Comparison with glm
%%		- Fibre optic data - event classification
%% 			- Comparison with glm
%% 		- NO2 data - event extraction

%% Future work and Conclusions

% =======================================================================
\section{Introduction}
% =======================================================================

Early detection and classification of emerging events in data streams is an important challenge in our data-rich world. Data streams may arise from many different applications including social media, Internet of Things, video surveillance, epidemiology and wireless sensors, to name a few. Often these applications are associated with different disciplines, where discipline-specific techniques have been developed, leading to little consensus on associated terminology across disciplines.

On the one hand, similar terms are used to describe different phenomena --- this is called the synonym problem \cite{zhou2014spatiotemporal}. On the other hand, different terms are used to describe similar phenomena --- the homonym problem \cite{zhou2014spatiotemporal}. This makes it difficult to objectively compare results and methods across disciplines. For example, the term ``event detection'' is used in video surveillance and video applications \cite{adam2008robust, ke2005efficient, medioni2001event}, social media \cite{weng2011event, li2012tedas, abdelhaq2013eventweet}, broadcast news stories \cite{allan1998line, li2005probabilistic} and wireless sensor networks \cite{yin2009spatio, mao2015online}. However, it is difficult to apply an event detection method used in one application to another domain as many contextual properties depend on the problem domain, resulting in a proliferation of custom-made algorithms. As a result, event detection has become a discipline-specific study with little overlap between different research problems.

Similarly spatio-temporal event detection, which is a popular sub-topic, has emerged from different applications such as river sensor networks \cite{mao2015online}, traffic data \cite{souto2016event}, wireless sensor networks \cite{mousavi2013spatio}, social networks \cite{cheng2014event} and epidemiology \cite{gomide2011dengue}. While spatio-temporal event detection has been studied extensively, early classification of spatio-temporal events has not received much attention. Mostly standard classification techniques are used on detected events where the focus has been on event detection \cite{kang2014detecting}.

%OK, so you are describing a situation where the feature vector can't really be computed accurately until the event is finished (so we can know it's shape etc.) but at earler times we have an incomplete/partial observation, and we want to know if we can use this to do early classifation of events (assuming that we have already been successful in early detection of events?)

We are interested in events that start, develop for some time, and stop at a certain time. Along with the event features, we use the ``age'' to describe these events. We note that the age and time of events are different quantities. The ``age'' of an event is the difference between the current time and the start time of the event.

For events with age-varying behaviour, standard classification techniques may not be suitable, especially if early detection is important. This is because the event features may change as the event progresses, such that an accurate feature vector can only be computed after the event is finished. For applications where early classification is important, this is not satisfactory as one cannot wait until the event finishes before classifying it. While the event is still progressing, we have access to incomplete/partial information, giving rise to a premature/incomplete feature vector. Our focus is on classifying events using this premature feature vector. To do this we incorporate age-varying coefficients in our model similar to other varying-coefficients models \cite{hastie1993varying}. A linear model with age-varying coefficients is given by
\begin{equation}\label{eq:Int1}
	y_t = a_0(t) + a_1(t) x_1(t) + \dots + a_b(t)x_b(t) + \varepsilon_t \, ,
\end{equation}
where $y_t$ is the output at age $t$, $a_i(t)$ are the age-varying coefficients, and $x_i(t)$ are the attributes of the event at age $t$. A logistic model with age-varying coefficients is given by Equations \eqref{eq:Int1} and \eqref{eq:Int2}:
\begin{equation}\label{eq:Int2}
	z_t = e^{y_t}/(1 + e^{y_t}) \, ,
\end{equation}
where $z_t$ is the probability of the event being of a given class. As an event develops, the features $x_i(t)$ change with the age of the event, while keeping the class label constant. Thus, it is clear that the coefficients $a_i(t)$ need to change with the age of the event.

At this point, we note that concept drift \cite{widmer1996learning, tsymbal2004problem,klinkenberg2000detecting,gama2014survey} or non-stationarity of data streams \cite{hulten2001mining, gama2010knowledge, gaber2005mining} is different from age-varying events. As we will show in Section~\ref{subsec:Synthetic}, age-varying events can occur in stationary data streams or data streams without any concept drift. This is because age-varying behaviour comprises change within the event as opposed to a change within the data stream. Even though non-stationarity and time-varying models have been studied in different contexts \cite{harvey1989time, wang1998cluster, hoover1998nonparametric}, they have not been explored in the context of premature event classification to the best of our knowledge.

%may last time-dependent class boundary is not encapsulated in standard classifiers.
% Our focus is on early classification of time dependent events. Specifically, early classification of spatio-temporal events with contiguous spatial dimensions.
%We note that age and time are different quantities. The age of an event is the time duration for which it has been growing, i.e. the difference between the current time and the birth time of the event, where as time refers to the current time. We investigate time-dependent models for spatio-temporal events that change with event-age.

We explore early event classification using two approaches that are based on different principles: 1) logistic regression; and 2) state-space models. These two approaches complement each other as logistic regression by itself is a static model more suited for stationary distributions, and a state space model coupled with a Kalman filter can update easily, which is advantageous for non-stationary data distributions. On the other hand, logistic regression may give better results than state space models for a stable data distribution.

Our proposed framework, depicted in Figure~\ref{fig:1} can be used in applications that give rise to spatio-temporal events with contiguous spatial dimensions. Typically, pre-processing is also a common step done before event extraction. However we do not focus on pre-processing data as techniques differ depending on the application. As event extraction is needed before classification, we also propose a simple method for event extraction. We make this work available in the R package \textit{eventstream} \cite{eventstream}.

\begin{figure}[!hb]
	\begin{tikzpicture}[node distance=2cm]
		\node (start) [standard] {Data Stream};
		\node (extract) [standard, right of=start, xshift=1.5cm] {Event \\ Extraction};
		\node (features) [standard, right of=extract, xshift=1.5cm] {Feature \\ Computation};
		\node (classify) [standard, right of=features, xshift=1.5cm] {Event \\ Classification};
		\node (output) [standard, right of=classify, xshift=1.5cm] {Output};

		\draw [arrow] (start) -- (extract);
		\draw [arrow] (extract) -- (features);
		\draw [arrow] (features) -- (classify);
		\draw [arrow] (classify) -- (output);
	\end{tikzpicture}
	\caption{Framework for event extraction and classification for spatio-temporal data.}
	\label{fig:1}
\end{figure}

Figure~\ref{fig:Real_World_Data} gives the heatmap of a dataset produced from a fibre optic cable. A pulse is periodically sent through the cable and this results in a data matrix where each horizontal row gives the strength of the signal at a fixed location $x_0$, and each vertical column gives the strength of the signal along the cable at a fixed time $t_0$. In this dataset the yellow parts represent high intensity values and the blue parts represent low intensity values.

\begin{figure}
%	\centering
	\subfloat[][]{
		\includegraphics[scale=0.9]{./Graphics/Real_World.pdf}
		\label{fig:Real_1}
	}
	\subfloat[][]{
		\includegraphics[scale=0.8]{./Graphics/Two_Event_Features.pdf}
		\label{fig:Real_Feat_1} % width=0.48\textwidth, height=0.2\textheight  width=0.48\linewidth
	}

	\caption{Figure~\ref{fig:Real_1} shows data from a fibre optic cable. We extract events  from this window and compute event features. We consider two events belonging to two different classes and an event feature that changes with event-age.  Figure~\ref{fig:Real_Feat_1} shows this event feature, which is the slope of a line fitted to the event signal values, and how it changes with event-age.}
	\label{fig:Real_World_Data}
\end{figure}



Fibre optic sensor cables are used in many applications including optical communications, detecting undersea cable faults \cite{jiang2009technological}, detecting oil leakages \cite{nikles2004leakage}, detecting intruders on secured premises \cite{griffiths1995developments}, monitoring health of infra-structure such as bridges and pipe-lines \cite{li2004recent}, to name a few. Events in these applications can often be grouped into two classes. For example, a cable lying on the sea bed can produce spatio-temporal events that are either cable faults (A), or non-fault events due to the activity in the ocean (B). Due to its sensitivity, fibre optic cables are also prone to noise. In a setting where early classification is important, we need to classify these events quickly, preferably while they are still ongoing.

In the dataset in Figure~\ref{fig:Real_World_Data}, events are seen in the lighter-coloured parts. The event approximately at location 30 between the time interval 45 to 60 is of class A while other events that appear between locations 150 and 400 are of class B. Due to the commercially sensitive nature of the dataset, we refrain from giving details about the actual application.

We will investigate early classification of spatio-temporal events using the framework in Figure~\ref{fig:1}. The remainder of the paper is organised as follows. Section~\ref{sec:EventExtract} discusses event extraction and feature computation, while Section~\ref{sec:Notation} introduces the notation for classification of age-varying events. In Sections~\ref{sec:ExtendedClassifier} and~\ref{sec:CascadedDLM} we introduce the age-varying events classifier and the cascaded non-Gaussian state space models respectively. %In Section~\ref{sec:Experiments} we discuss the synthetic datasets, and the results of the two early detection methods.
In Section~\ref{sec:Experiments} we discuss results of our framework using synthetic data and two real applications. The first application is associated with the dataset in Figure~\ref{fig:Real_World_Data} and the second application is on Nitrogen dioxide $(\text{NO}_2)$ data from NASA's NEO \cite{OMINO2} website. Finally, in Section~\ref{sec:Conclusions}, we present our conclusions and discuss future work.

% =======================================================================
% =======================================================================
\section{Event extraction and feature computation} \label{sec:EventExtract}
% =======================================================================

\subsection{Event extraction}

We extract events from data streams of 2 or 3 dimensions; i.e. 1 time dimension and either 1 or 2 spatial dimensions. We employ a simple method for event extraction using DBSCAN \cite{ester1996density}, which is a density based clustering algorithm. Our event extraction algorithm is explained in Algorithm~\ref{algo:events_extraction}.

\DontPrintSemicolon
\begin{algorithm}\fontsize{9}{10}\selectfont
	\SetKwInOut{Input}{input~~~}
	\SetKwInOut{Output}{output}
	\Input{~a 2 or 3-dimensional array $A_{n\times m}$ or $A_{n\times m \times k}$ , and parameters $\alpha$, $\epsilon$ and \textit{minPts}.}
	\Output{~events $A|_S \subset A$ and the event ids for each $s_{ij} \in A|_S$ (or $s_{ijk}$ if $A$ is 3-dimensional).}
	Let $a_{ij}$ be the signal value at $(i, j)$ position of $A$, if $A$ is 2-dimensional and $a_{ijk}$ be the signal value at $(i, j, k)$ position of $A$ if $A$ is $3-$ dimensional. \\
	Let $q$ denote the $\alpha$-percentile of the signal values of $A$. \\
	$S = \{ (i,j) \mid a_{ij} > q \}$ for 2-dimensional $A$ and
	$S = \{ (i,j, k) \mid a_{ijk} > q \}$ for 3-dimensional $A$. \\
	$ B = A|_S$, i.e., signal values of $A$ in $S$ locations. \\
	Using DBSCAN cluster B using $\epsilon$ and \textit{minPts}. \\
	This clustering gives each $s \in A|_S$ a cluster id. \\
	%Convert $A$ to a mesh-grid format $B$, i.e. $B$ contains 2D/3D grid coordinates and the value of each cell of $A$. \;
	%Of the cell values, get the positions and values of $B$ greater than $\alpha^{\text{th}}$ percentile. \;
	%Using DBSCAN cluster the chosen cells using $\epsilon$ and \textit{minPts}. \;

	Consider each cluster as an event. \,
	\caption{\itshape Extract events from a dataset or window.}
	\label{algo:events_extraction}
\end{algorithm}

Figure~\ref{fig:blobs_A_B} shows a  synthetic dataset generated from the package \textit{eventstream}, along with the events extracted using Algorithm~\ref{algo:events_extraction}.

\begin{figure}
	\centering
	\subfloat[][]{
		\includegraphics[width=0.48\textwidth]{./Graphics/gen_data.pdf} % 3_B_Blobs.pdf ./Graphics/Extracted_Blobs_A_file.pdf
		\label{fig:Class_A_1}
	}%
	\subfloat[][]{
		\includegraphics[width=0.48\textwidth]{./Graphics/events_extracted.pdf} % Extracted_Blobs_Class_B_file.pdf
		\label{fig:Class_B_1}
	}
	\caption{A synthetic dataset from \textit{eventstream} and the events extracted from it using $\alpha=0.95$, $\epsilon = 5$ and {\it minPts} $= 10$.}
	\label{fig:blobs_A_B}
\end{figure}

\subsection{Features}\label{sec:Featurelist}

As we work with a data stream, we use a moving window model in our experiments. We extract events from data in the current window and compute features for these events. The feature set comprises some basic features such as length and width of each event, and some other features that compute the intensity of each event relative to the background. The  ``Relative to the background'' features are equivalent to a family of signal to noise ratio features and are motivated from the fibre optic application (see Figure~\ref{fig:1}).

To compute the SNR family of features  we use smoothing splines and thus they are only computed for two-dimensional data streams due to ease of computation.  Using a small portion from the beginning of each window, which correspond to the recent past, we compute the mean, median, IQR and standard deviation for each location. Using these values at each location, we compute four smoothing splines. The objective is to have the background mean, median, IQR and standard deviation pixel value for each location. The median and IQR splines from a small window in Figure~\ref{fig:Im_Splines} are shown in Figures~\ref{fig:Spline_Median} and~\ref{fig:Spline_IQR}. %This enables the computation of event strength or intensity relative to the background.

\begin{figure}[!hb]
	\centering
	\subfloat[][]{
		\includegraphics[width=0.32\textwidth]{./Graphics/Image_for_Splines.pdf} %
		\label{fig:Im_Splines}
	}%
	\subfloat[][]{
		\includegraphics[width=0.32\textwidth]{./Graphics/Spline_Median.pdf}
		\label{fig:Spline_Median}
	}
	\subfloat[][]{
		\includegraphics[width=0.32\textwidth]{./Graphics/Spline_IQR.pdf}
		\label{fig:Spline_IQR}
	}
	\caption{The initial portion of a window and the resulting median and IQR splines.}
	\label{fig:splines}
\end{figure}

For two-dimensional events we compute the following features:
\begin{compactenum}
	\item Number of cells/pixels in event
	\item Length of event
	\item Width of event
	\item Length to width ratio of event
	\item Centroid \\
	      The centroid is used to compute other features which are relative to the event. It is not used in event classification.
	\item Sum of signal-values of cells in event
	\item Mean signal-value of event
	\item Standard deviation of signal-values of event
	\item Slope of the fitted line $\zeta$ \\
	      The average signal value at each time of the event is computed and a line $\zeta$ is fitted to the average values. The slope of the fitted line $\zeta$ is a feature of interest .
	\item Linear and quadratic coefficients of a fitted parabola $p$ \\
	      The average signal value at each time of the event is computed and a parabola $p$ is fitted to the average values. The linear and quadratic coefficients of the fitted parabola $p$ are features of interest.
	\item $n$ standard deviations from the mean \\
	      The proportion of event cells/pixels that has signal-values greater than $n$ global standard deviations from the global mean for $n \in \{2, 3, 4\}$.
	\item $n$ local IQR from local median \\
	      %We use a small portion from the beginning of each window to compute smoothing splines which are indicative of the level of activity or noise for each location. Four smoothing splines are computed using the horizontal row means, medians, IQR values and standard deviation values. For example from the initial slice of data, the IQR is computed for each horizontal row and a smoothing spline is fitted to these IQR values giving the IQR spline.
	      The value of the median smoothing spline at each event centroid is used as the local median for that event. Similarly, the value of the IQR smoothing spline at each event centroid is used as the local IQR for that event. This feature gives the proportion of event pixels/cells that has signal-values greater than $n$ local IQRs from the local median for $n \in \{ 5, \dots, 8 \} $
	      %	\item $n$ standard deviation from spline mean \\
	      %	Similar to the previous feature, this feature gives the proportion of event pixels/cells that has values greater than $n$ local means from the local standard deviation for $n \in \{2, \dots, 7 \}$.
	\item Local IQRs from local median \\
	      Let us denote the 75th percentile of the event signal value by $x$. This feature gives the number of local IQRs for which $x$ is greater than the local median. Both local IQR and local median are computed using splines described above.
	\item Local standard deviation from local mean \\
	      Similar to the previous feature, our $x$ is the 80th percentile of the event signal value. Here we compute the number of local standard deviations for which $x$ is greater than the local mean.
\end{compactenum}

For three-dimensional data streams we compute a subset of the above features. In particular, we compute features 1--8 from the above list and an equivalent of feature 14 using the global standard deviation and the global mean.  These features now provide a compact way to represent a data stream and the embedded events, summarising salient properties of the time window in terms of event signal strength and shape. This summary becomes input to a classifier to identify types of events.


% =======================================================================
% =======================================================================
\section{Partial/incomplete observations} \label{sec:Notation}
% =======================================================================

In the classical setting, a classification problem comprises observations $(\bm{x}_i, y_i)$ for $i \in 1,\dots, N$ where $\bm{x}_i \in \mathbb{R}^b$ is the attribute vector of the $i$th observation and $y_i$ is its class label. The task of the classifier is to learn the class boundary by using the given set of observations. Then for any new observation $\bm{x}_j$, the classifier can predict its class label using the learned class boundaries. Let us call this a \textbf{standard classifier}.

Standard classifiers have been widely popular in diverse fields of study and practice. However, they are not without limitations. One of the limitations is that once a classifier is trained, it has fixed class boundaries. If the new data is different from the data learned by the classifier, the output of the classifier is of little use. This is particularly the case in data-streaming scenarios, where data distributions are non-stationary (sometimes also referred to as concept drift). It is necessary for a classifier to re-adjust its class boundaries when faced with non-stationarity. The literature on adapting or evolving classifiers is significant \cite{duchi2011adaptive, dabbagh2005online, frey1991letter, giacinto1997adaptive, nishida2005ace,alippi2008just, alippi2008just2}. Let us call these classifiers \textbf{evolving classifiers}.

Now, consider the case when a new observation is not made available at once but gradually, where we get partial information about the new observation and the amount of partial information increases with time. This is the case for events described in Section~\ref{sec:EventExtract}. Let $\bm{x}_j$ be a new observation which becomes available partially via the following finite sequence of partial observations $\{\bm{p}^j_{t_1},\bm{p}^j_{t_2}, \bm{p}^j_{t_3}, \dots, \bm{p}^j_{t_n}\}$. Here the partial observation of $\bm{x}_j$ at age $t_k$ is denoted by $\bm{p}^j_{t_k}$ and $\bm{p}^j_{t_n} = \bm{x}_j$ with $t_1 < t_2 < \dots < t_n$. We differentiate between the time and the age of a partial observation. A partial observation that begins at time $t =t_1$ has age $0$ at time $t_1$, and at time $t = t_2$ it has age $t_2 - t_1$.

We consider the question ``how can we classify partial observations?'' If one trains a single standard classifier on all partial observations, it may be optimal for a certain set of partial observations $p_{t_k}$ at a given age $t_k$, but not all partial observations, because partial observations change with time. If one waits until the partial observation has formed into a full observation $\bm{x}_j$, then a standard classifier can be used. However, for some applications such as intrusion detection it might be too late to wait until the full observation has formed. One option is to have a series of standard classifiers $\{C_{t_i}\}_{i=1}^n$ each trained on partial observations $p_{t_i}$. When a new observation gradually arrives in the form of a sequence of partial observations $\{\bm{p}^k_{t_1},\bm{p}^k_{t_2}, \bm{p}^k_{t_3}, \dots, \bm{p}^k_{t_n}\}$, the classifier $C_{t_i}$ can be used on $\bm{p}^k_{t_i}$. Thus, as the partial observation grows, we have a growing prediction $\{y_{t_1}, y_{t_2}, \dots, y_{t_n}\}$ of the class label. More importantly, we do not need to wait until the partial observation matures to a full observation before making a prediction.

However, having a series of classifiers independent of each other is sub-optimal because each classifier is only trained on a portion of the data, i.e. it is trained on  individual snapshots of events at different ages. By linking event snapshots of different event-ages in an appropriate way,  better predictions can be achieved.  The age-varying events classifier described in the next section addresses this limitation, while giving a growing prediction.

%Here we note that one does not need to have $t_n$ number of classifiers, but can do with a fewer number of classifiers such that for partial observations with age $t_1 \leq t \leq t_i$ classifier $C_1$ is used, and for partial observations with age $t_i \leq t \leq t_j$ classifier $C_2$ is used and so on. Here $t_1 < t_i < t_j $.

%Partial observations can occur in data streaming scenarios when raw data is processed in some manner to extract observations of interest which are then fed to a classifier. As discussed earlier, these processed observations are sometimes referred to as events or episodes. But as the word ``event'' describes different things in different settings we will refrain from using this word. %In such a scenario, observations that are input to the classifier are not raw data, but constructed from raw data.

% =======================================================================
% =======================================================================
\section{Age-varying events classifier}\label{sec:ExtendedClassifier}
% =======================================================================

%Another option is to extend a standard classifier in time in the following manner.
Let the standard classifier minimize the loss function given by $\mathscr{L}$, i.e.
$$
  \argmin_\beta \frac{1}{N}\sum_{i=1}^N \mathscr{L}(\bm{x}_i,y_i;\beta) \, ,
$$
where $\beta = \left( \beta_0, \beta_1, \dots, \beta_l \right)$ and $(\bm{x}_i, y_i)$ are observations for $i \in \{ 1, \dots, N \}$. By extending a classifier to take in partial observations, we initially minimize the following loss function,
$$
  \argmin_{\tilde{\beta}} \frac{1}{nN} \sum_{i=1}^N \sum_{j=1}^n \mathscr{L} ( \bm{p}^i_{t_j},y_i;\tilde{\beta}) \, ,
$$
where $\tilde{\beta} = \{ \tilde{\beta}_{jk} \}$ for $j \in \{1, \dots, n\}$, \, $k \in \{0, 1, \dots, l \}$ and $\bm{x}_j$ gradually becomes available as $\{\bm{p}^j_{t_1},\bm{p}^j_{t_2}, \bm{p}^j_{t_3}, \dots, \bm{p}^j_{t_n} \}$ with $\bm{x}_j = \bm{p}_{t_n}^j$ . For a given $j$, $y_j$ is the class label of $\bm{x}_j$, and so for all $k$, $\bm{p}^j_{t_k}$ have the same $y_j$. Let us call this initial extended classifier $\mathscr{E}$.

We note that $\mathscr{E}$ is equivalent to a series of $n$ classifiers $\{C_{t_j}\}_{j=1}^n $. To show this, first note that $C_{t_j}$ minimizes the loss function
$$
  \argmin_{\tilde{\beta}_j} \frac{1}{N} \sum_{i=1}^N \mathscr{L} ( \bm{p}^i_{t_j},y_i;\tilde{\beta}_j ) \, ,
$$
with $\tilde{\beta}_j = ( \tilde{\beta}_{j0}, \tilde{\beta}_{j1}, \tilde{\beta}_{j2}, \dots, \tilde{\beta}_{jl})$. Also,
\begin{equation}\label{eq:4_1}
	\argmin_{\tilde{\beta}} \sum_{i=1}^N \sum_{j=1}^n \mathscr{L} ( \bm{p}^i_{t_j},y_i;\tilde{\beta}) = \argmin_{\tilde{\beta}} \sum_{j=1}^n \sum_{i=1}^N \mathscr{L} ( \bm{p}^i_{t_j},y_i;\tilde{\beta} ) \, .
\end{equation}
In addition $\{\bm{p}^i_{t_j}\}_{i=1}^N$ for a fixed age $t_j$ only affects $\tilde{\beta}_j$. Therefore the matrix $\tilde{\beta}$ can be computed row by row by minimizing $\sum_{i=1}^N \mathscr{L} \left( \bm{p}^i_{t_j},y_i;\tilde{\beta}_j \right)$ for each $j$. Thus, we can write
\begin{equation}\label{eq:4_2}
	\argmin_{\tilde{\beta}} \sum_{j=1}^n \sum_{i=1}^N \mathscr{L} \left( \bm{p}^i_{t_j},y_i;\tilde{\beta} \right) = \left[\argmin_{\tilde{\beta}_1} \sum_{i=1}^N \mathscr{L} \left( \bm{p}^i_{t_1},y_i;\tilde{\beta}_1 \right) \cdots \argmin_{\tilde{\beta}_n} \sum_{i=1}^N \mathscr{L} \left( \bm{p}^i_{t_n},y_i;\tilde{\beta}_n \right) \right]^T \!\!.
\end{equation}
%$$ \sum_{j=1}^n \left( \argmin_{\tilde{\beta}_j} \sum_{i=1}^N \mathscr{L} \left( \bm{p}^i_{t_j},y_i;\tilde{\beta}_j \right) \right) $$
From equations \eqref{eq:4_1} and \eqref{eq:4_2} we see that the initial extended classifier $\mathscr{E}$ is equivalent to a series of $n$ classifiers $\{C_{t_j}\}_{j=1}^n $.

Having $n$ independent classifiers $\left\{C_{t_j}\right\}_{j=1}^n$ will make the class boundaries for each classifier independent of each other. As $\bm{x}_i \in \mathbb{R}^b$, the class boundary of $C_{t_j}$ lives in the ambient space $\mathbb{R}^{b}$. Let us denote the class boundary of $C_{t_j}$ by $B_j$ and the class boundary of $\mathscr{E}$ by $B_{\mathscr{E}}$. As $n$ independent classifiers $\left \{C_{t_j} \right \}_{j=1}^n$ in totality are equivalent to $\mathscr{E}$, it follows that the class boundary $B_{\mathscr{E}}$ comprises the set $\left\{ B_j \right\}_{j=1}^n $. By stacking $B_j$ in the order of increasing $j$, i.e. by considering the set $\left \{ \left(t_j, B_j \right) \right \}_{j=1}^n $, we can visualize a continuous class boundary that connects the set $\left\{ B_j \right\}_{j=1}^n$ with increasing $t_j$. This continuous class boundary lives in $\mathbb{R}^{b+1}$. Thus, the set $\left \{ \left(t_j, B_j \right) \right \}_{j=1}^n$ can be viewed as a discrete approximation of a continuous class boundary. Therefore, $B_{\mathscr{E}}$ can be viewed as living in $\mathbb{R}^{b+1}$.

In addition, it is desirable if $B_{\mathscr{E}}$ has a certain level of smoothness, because it aids convergence of early classification based on partial observations to  full classification based on full observations. To incorporate smoothness in $B_{\mathscr{E}}$, we modify the original loss function by including an $L_2$ penalty term as follows:
\begin{equation}\label{eq:ExtParCl_1}
	\varphi\left(\tilde{\beta}, \lambda \right) = \frac{1}{nN} \sum_{i=1}^N \sum_{j=1}^n \mathscr{L} \left( \bm{p}^i_{t_j},y_i;\tilde{\beta} \right) + \lambda \sum_{k=1}^l\sum_{j=1}^{n-1} \left(\tilde{\beta}_{j+1,k} - \tilde{\beta}_{j,k} \right)^2 \, ,
\end{equation}
for some $\lambda >0 $. The constant $\lambda$ is a parameter that can be specified. Recall that $\tilde{\beta}_j = \left( \tilde{\beta}_{j1}, \tilde{\beta}_{j2}, \dots, \tilde{\beta}_{jl} \right)$ relates to partial observations $\{ \bm{p}^i_{t_j}\}_{i=1}^N$ for a fixed $t_j$, i.e. $\tilde{\beta}_{j1}$ is the coefficient of the first covariate at age $t_j$ and $\tilde{\beta}_{j2}$ the coefficient of the second covariate at age $t_j$. Thus the penalty term $(\tilde{\beta}_{j+1,k} - \tilde{\beta}_{j,k})^2$ takes coefficients for the $k$th covariate at ages $t_j$ and $t_{j+1}$ and penalizes the difference, enforcing a certain smoothness in event-age. We minimize the loss function described by equation \eqref{eq:ExtParCl_1} in our proposed classifier. As we use this classifier on premature events, we call this the \textit{age-varying events classifier}.
\begin{equation}\label{eq:ExtParCl_2}
	\argmin_{\tilde{\beta}} \varphi\left(\tilde{\beta}, \lambda \right) = 	\argmin_{\tilde{\beta}} \left( \frac{1}{nN} \sum_{i=1}^N \sum_{j=1}^n \mathscr{L} ( \bm{p}^i_{t_j},y_i;\tilde{\beta} ) + \lambda \sum_{k=1}^l\sum_{j=1}^{n-1} (\tilde{\beta}_{j+1,k} - \tilde{\beta}_{j,k} )^2 \right) \, .
\end{equation}

This is a general formulation for an age-varying events classifier, i.e. any loss function $\mathscr{L}$ can be used in equation \eqref{eq:ExtParCl_2}. Without loss of generality, we implement the age-varying events classifier for logistic regression in our R package \textit{eventstream}. For logistic regression \cite{friedman2001elements} the loss function $\mathscr{L}$ is given by
\begin{equation}\label{eq:LogReg}
	\mathscr{L} (\bm{p}^i_{t_j},y_i;\tilde{\beta}) = - y_i \left( [ 1 \,\, ( \bm{p}^i_{t_j})^T] \tilde{\beta}_{j} \right) + \log \left( 1+ \exp\big\{[ 1 \, \,( \bm{p}^i_{t_j})^T ] \tilde{\beta}_{j}\big\} \right) \, .
\end{equation}
Here the vector $[1 \,\, ( \bm{p}^i_{t_j})^T]$ denotes the concatenation of the vector $\bm{p}^i_{t_j}$ with the constant $1$ to account for the intercept. %We refer to our age-varying events classifier for logistic regression by $\mathscr{P}$ in the following sections.

Next we explore a non-Gaussian state space model for partial observations.

% =======================================================================
% =======================================================================
\section{Cascaded non-Gaussian state space models}\label{sec:CascadedDLM}
% =======================================================================

We use non-Gaussian state space models as described in \cite{durbin1998time} and \cite{mccormick2012dynamic} to model partial observations, and the implementation in  R package \textit{dma} \cite{dma}. We start with the linear Gaussian state space models which are given by the following equations :
\begin{align}
	y_t          & = Z_t \alpha_t + \varepsilon_t \, , \label{eq:DLM_1} \\
	\alpha_{t+1} & = T_t \alpha_t + R_t \eta_t \, , \label{eq:DLM_2}
\end{align}
where $\varepsilon_t \in \mathcal{N} \left(0, H_t\right)$ and $\eta_t \in \mathcal{N} \left(0, Q_t\right)$. %and $\alpha_1 \in \left(0, Q_t\right)$.
Here $y_t$ is an $N$-dimensional vector of observations and $\alpha_t$ is a $(l+1)$-dimensional state vector. The matrices $Z_t$, $T_t$, $R_t$, $H_t$ and $Q_t$ are known at the beginning.  Equation \eqref{eq:DLM_1} is called the ``observation equation'' and equation \eqref{eq:DLM_2}  the ``state equation''. When using state space models for dynamic regression, $\alpha_t$ denotes the coefficients which update dynamically.

%If the observations $y_t$ come from an exponential family distribution, the equations change as follows:
%\begin{align}
%	\theta_t     & = Z_t \alpha_t \, , \label{eq:GDLM_2}              \\
%	\alpha_{t+1} & = T_t \alpha_t + R_t \eta_t \, , \label{eq:GDLM_3}
%\end{align}
%with $\E(y_t) = \mu_t$ and $f(\mu_t) = \theta_t$ where $f$ is an appropriate link function. For a binomial distribution, the logit function $f(t)=\log(t/(1-t))$ is used as the link.

If $y_t$ is a binary response such that
$$ y_t \sim \text{Bernoulli}(p_t) \, , $$
equation \eqref{eq:DLM_1} changes as

\begin{equation}\label{eq:GDLM_2}
	\text{logit}(p_t) =   Z_t \alpha_t \, ,              \\
\end{equation}
with logit function $f(t)=\log(t/(1-t))$, while keeping equation \eqref{eq:DLM_2} unchanged.

\subsection{Modelling partial observations}

We are interested in a growing prediction $\{\theta^j_{t_1}, \theta^j_{t_2}, \dots, \theta^j_{t_n} \}$ for partial observations $\{\bm{p}^j_{t_1},\bm{p}^j_{t_2}, \bm{p}^j_{t_3}, \ldots \bm{p}^j_{t_n}\}$ belonging to the same complete observation. Again, we note that for partial observations $\{\bm{p}^j_{t_1},\bm{p}^j_{t_2}, \bm{p}^j_{t_3}, \ldots \bm{p}^j_{t_n}\}$, the time stamps $(t_1, t_2, \ldots, t_n)$ denote the ages of the partial observations, not the time of occurrence. % As such, we want the prediction $\theta^j_{t_k}$ to be more confident as the age ${t_k}$ grows. This is what we mean by a growing prediction$\left\{ \theta^j_{t_1}, \theta^j_{t_2}, \dots, \theta^j_{t_n} \right\}$ .

%$\left\{ \theta^j_{t_1}, \theta^j_{t_2}, \dots, \theta^j_{t_n} \right\}$
% $\left\{ y^j_{t_1}, y^j_{t_2}, \dots, y^j_{t_n} \right\}$
As state space models generally deal with time, for each age $t_k$ we use a separate state space model. However, for some cases it may benefit if there is communication between the models regarding the same observation. To add communication between the models, we bundle the partial observation $\bm{p}^j_{t_k}$ with the prediction output $\theta^j_{t_{k-1}}$ of the lower age model and use it as input for the age $t_k$ model. % For example the prediction of partial observation $\bm{p}^j_{t_1}$ is given by one state space model and the prediction of $\bm{p}^j_{t_2}$ is given by another model without any link between the two.

To explain this further, let us define $\bm{P}_{t_k} = \left[ \bm{p}^1_{t_k}\, \bm{p}^2_{t_k}\, \cdots \, \bm{p}^N_{t_k} \right]^T$ - an $l \times N$ matrix containing all partial observations of age $t_k$, $\bm{1} = \left[ 1, 1, \dots, 1\right]^T$ and $\theta_{t_k} = \left[\theta^1_{t_k}, \theta^2_{t_k}, \dots, \theta^N_{t_k} \right]^T$ - the logit predictions of the partial observations in $\bm{P}_{t_k}$, where $\bm{p}^j_{t_k}$ is an $l$-vector and $N$ denotes the number of complete observations. %Here $\bm{P}_{t_k}$ is an $l \times N$ matrix containing all partial observations of age $t_k$.
Then the equation corresponding to equation \eqref{eq:GDLM_2} for the initial model for age $t_1$ can be written as
% and $\bm{y}_{t_k} = \left\{ y^1_{t_k}, y^2_{t_k}, \dots, y^N_{t_k} \right\}$ and $\bm{y}_{t_k}$ an $N \times 1$ vector containing all class labels.
\begin{align}
	\theta_{t_1} & = Z_{t_1} \alpha_{t_1} \, . \label{eq:OurMod2}                     \\
	\text{with} \qquad
       Z_{t_1} & = [ \bm{1} \, \, \bm{P}_{t_1} ] \, , \label{eq:OurMod1}
\end{align}
Using the output $\theta_{t_1}$, we write the model for age $t_2$ as
\begin{align}
	Z_{t_2}      & = [ \bm{1} \, \, \bm{P}_{t_2} \, \, \theta_{t_1}] \, ,\label{eq:OurMod3} \\
	\theta_{t_2} & = Z_{t_2} \alpha_{t_2} \, . \label{eq:OurMod4}
\end{align}
Building in this way we obtain
\begin{align}
	Z_{t_k}      & = [ \bm{1} \, \, \bm{P}_{t_k} \, \, \theta_{k_{t-1}}] \, , \label{eq:OurMod5} \\
	\theta_{t_k} & = Z_{t_k} \alpha_{t_k} \, . \label{eq:OurMod6}
\end{align}
The equations \eqref{eq:OurMod5}, \eqref{eq:OurMod6} and \eqref{eq:DLM_2} describe the model for the partial observations at age $t_k$. Let us denote the model which describes the partial observations of age $t_k$ by $\Lambda_k$ and the predicted output using the inverse link function by $\hat{\bm{y}}_{t_k}$. Then Figure~\ref{fig:PODLM} gives the structure of our partial observations model.

\begin{figure}[!hb]
	\centering
	\includegraphics[clip=true,scale=0.8]{./Graphics/Lots_of_circles_3.pdf}
	\caption{Structure of the partial observations model with communication.}
	\label{fig:PODLM}
\end{figure}

However, communication between models $\Lambda_{k-1}$ and $\Lambda_k$ may not improve the overall accuracy for every partial observations problem. For example, for a class imbalanced problem, a small number of incorrect predictions in $\theta_{t_{k-1}}$ can have an adverse effect on the model $\Lambda_k$. Therefore, for some instances such as class imbalanced datasets, it is desirable to have models $\Lambda_k$ without communication between the models, as shown in Figure~\ref{fig:PODLM2}.

\begin{figure}[!hb]
	\centering
	\includegraphics[clip=true,scale=0.8]{./Graphics/Lots_of_circles_4.pdf}
	\caption{Structure of the partial observations model without communication.}
	\label{fig:PODLM2}
\end{figure}


% =======================================================================
\section{Applications}\label{sec:Experiments}
% =======================================================================

We use age-varying events classifiers and cascaded state space models to classify events in synthetic and real data streams. The R code applicable to this section is available in the supplementary material.

\subsection{Synthetic data}\label{subsec:Synthetic}

The synthetic data used in this section can be generated from the R package \textit{eventstream}. The synthetic data contains events of two classes: A and B. All events belonging to class A look similar, that is they have one single non-standard shape or visual pattern. In contrast, events belonging to class B can have one of three different non-standard shapes, including the shape of events of class A. The use of three different shapes for class B events and one shape for class A events (with that shape being similar to some of those of class B) was motivated from the real application.

Figure~\ref{fig:Class_A} contains two events of class A, and Figure~\ref{fig:Class_B} contains 3 events of class B. The shapes are labelled as $1$, $2$ or $3$ in both Figures~\ref{fig:Class_A} and~\ref{fig:Class_B}, with shape $1$ being the common shape.

%Motivated by the real world dataset shown in Figure~\ref{fig:Real_World_Data} we create a set of synthetic files which mimic some properties of the real world dataset. Each file is a $350 \times 250$ two-dimensional array of cells/pixels. Files contain events from either one or both classes, namely A and B.
%[trim=0.2cm 0.7cm 1.2cm 2.4cm, clip=true, width=0.48\textwidth]

\begin{figure}[!hbt]
	\centering
	\subfloat[][]{
		\includegraphics[width=0.32\textwidth]{./Graphics/2_A_Blobs_labels.pdf}
		\label{fig:Class_A} % width=0.48\textwidth, height = 0.48\textheight
	}%
	\subfloat[][]{
		\includegraphics[width=0.32\textwidth]{./Graphics/3_B_Blobs_labels.pdf}
		\label{fig:Class_B}
	}
	\caption{Class A events in Figure~\ref{fig:Class_A} and Class B events in Figure~\ref{fig:Class_B}.}
	\label{fig:Classes_A_And_B}
\end{figure}

The number of events of class A and B, and their positions, are randomly generated. The other difference between the events of class A and B, apart from the shape, is that values of the pixels belonging to events of class A and B come from different probability distributions. For both classes the intensity of pixel values increase linearly with the age of the event. We list the differences between class A and B events in Table~\ref{tab:DiffClassAandB}.

\begin{table}[!ht]
	\centering
	\begin{tabular}{lll}
		\toprule
    Feature                                   & Class A value distribution & Class B value distribution \\
		\midrule
    Starting cell/pixel values                & $\mathcal{N}(4,3)$         & $\mathcal{N}(2, 3)$                  \\
    Ending cell/pixel values                  & $\mathcal{N}(8,3)$         & $\mathcal{N}(4, 3)$                  \\
    Maximum age of event - shape 1            & $\mathcal{U}(20,30)$       & $\mathcal{U}(20,30)$                 \\
    Maximum age of event - shape 2            & -                          & $\mathcal{U}(100,150)$               \\
    Maximum age of event - shape 3            & -                          & $\mathcal{U}(100,150)$               \\
    Maximum location width of event - shape 1 & $\mathcal{U}(20,26)$       & $\mathcal{U}(20,26)$                 \\
    Maximum location width of event - shape 2 & -                          & $\mathcal{U}(30,38)$                 \\
    Maximum location width of event - shape 3 & -                          & $\mathcal{U}(50,58)$                 \\
		\bottomrule
	\end{tabular}
	 \caption{Differences in class A and class B events.}\label{tab:DiffClassAandB}
\end{table}

We classify the extracted events from synthetic data using: 1) the age-varying events classifier; 2) cascaded state space models; and 3) logistic regression. We use the same feature vector to summarise the extracted events. Figure~\ref{fig:TwoWindows} shows snapshots of the data and the extracted events for two time windows.

\begin{figure}
	\centering
	\subfloat[][]{
		\includegraphics[width=0.48\textwidth]{./Graphics/Events_100040.pdf}
		%\label{fig:Window_1} % width=0.48\textwidth, height = 0.48\textheight
	}%
	\subfloat[][]{
		\includegraphics[width=0.48\textwidth]{./Graphics/Events_100044.pdf}
		%\label{fig:Window_2}
	}
	\caption{Two windows of data and extracted events.}
	\label{fig:TwoWindows}
\end{figure}


\begin{figure}[!htb]
	\centering
	\subfloat[][]{
		\includegraphics[width=0.42\textwidth]{./Graphics/3_Classifiers_1.pdf}
		\label{fig:Facet1} % width=0.48\textwidth, height = 0.48\textheight
	}%
	\subfloat[][]{
		\includegraphics[width=0.42\textwidth]{./Graphics/3_Classifiers_2.pdf}
		\label{fig:Facet2}
	}
	\caption{Accuracy of the 3 classifiers over 5 repeats grouped by classifier in Figure~\ref{fig:Facet1} and by repetition in Figure~\ref{fig:Facet2}.}
	\label{fig:3Classifiers}
\end{figure}

We repeat each experiment $5$ times with data streams generated with different seeds. For each experiment we generate a data stream of dimension $3500 \times 250$ of which $80\%$ $(2800 \times 250)$ is used for training and the remaining $20\%$ for testing. For synthetic data classification, we do not use features which were motivated from the real example, i.e. we do not use features 11 and 12 from the list in Section~\ref{sec:Featurelist}, for computational efficiency. In addition, the centroid is not used in any classification task. As class A events can have a maximum age of $30$, we use $4$ event ages for the classification tasks at $t = 8, 16, 24$ and $32$ time units. That is, event features are calculated at these ages. We use a moving window model of dimension $200 \times 250$ which moves by a step of $8 \times 250$.

Figure~\ref{fig:3Classifiers} shows the accuracy of the age-varying events classifier, state space model and logistic regression classifier over 5 repetitions.  From Figure~\ref{fig:Facet2} we see that for each repetition either the age-varying events classifier or the state space model surpasses the logistic regression classifier. From Figure~\ref{fig:Facet1} we see that the age-varying events classifier performs better on average than the other two classifiers for synthetic data. Table~\ref{tab:Results_Synthetic} gives the average test set accuracy and standard deviation results for the 3 classifiers, which confirm these observations. Also we see that all 3 classifiers improve their average accuracy levels with the age of the events.

\begin{table}[!ht]
  \centering
  \begin{tabular}{lccccccccc}
    \toprule
    Classifier                   & \multicolumn{4}{c}{Accuracy} & & \multicolumn{4}{c}{Standard deviation} \\
    \cmidrule{2-5} \cmidrule{7-10}
                                 & $t_1$ & $t_2$ & $t_3$ & $t_4$   & & $t_1$ & $t_2$ & $t_3$ & $t_4$\\
    \midrule
    Age-varying events classifier & 81    & 90    & 93    & 92  &  & 9  & 8  &  7  & 5 \\
    Cascaded state space models  & 87    & 89    & 89    & 90   & & 5  & 4  &  5  & 2\\
    Logistic regression          & 75    & 78    & 77    & 78   & & 12  & 11 & 10 & 9 \\
    \bottomrule
  \end{tabular}
    \caption{Average test set accuracy and standard deviation $(\%)$ over $5$ repetitions.}\label{tab:Results_Synthetic}
\end{table}
% 5 repetitions
% glm - logistic regression
% developing events classifier
% cascaded dma
% for all training 8 x 350 blocks
% testing 2 x 350 blocks

\subsection{Fibre optic cable data}\label{sec:FibreOpticExperiment}

The data for the first real application is from a fibre optic cable, and is shown in Figure~\ref{fig:Real_World_Data_Stream}.  The data set is available in the R package \textit{eventstream}. Again, for commercially sensitive reasons, we cannot provide more information about the application. The data set has dimensions $379 \times 587$, with class A events labelled with letter \textbf{A}. The extracted events have a maximum age of 40 time units, so we use a window model with a window size $40 \times 587$ and a step size $10 \times 587$ to extract events and compute features.

\begin{figure}[!b]
	\centering
	\includegraphics[width=0.6\textwidth]{./Graphics/Real_World_stream.pdf}
	\caption{Data stream from a fibre optic cable.}
	\label{fig:Real_World_Data_Stream}
\end{figure}

Similar to the synthetic data stream, we classify the extracted event features using 3 classifiers. The main difference is that the real data stream has a much smaller number of class A events compared to class B events. Due to this class imbalance, we configure the state space models for real data differently from those used for the synthetic data. First, we use the state space models structure depicted in Figure~\ref{fig:PODLM2}. Second, as there are only $4$ class A events, there is not enough class A data to update the state space models regularly. Consequently, we use the state space models for training and prediction without the updating step given in equation \eqref{eq:DLM_2}.

We use $4$-fold cross validation because there are 4 class A events, with event ages $t = 10, 20, 30$ and $40$, because the maximum event-age is $40$ time units. The age-varying events, logistic regression and the state space models classifiers are trained on a data stream comprising 3 training folds, and tested on the remaining fold.

We report additional accuracy measures that are designed for imbalanced datasets. These metrics are positive predictive value (PPV), negative predictive value (NPV) and area under the receiver operator characteristic curve (AUC). We give the definitions of these metrics below:
$$
  \text{Positive predictive value (PPV)} = \frac{ \text{Number of true positives} }{ \text{Number of predicted positives} } \, ,
$$
$$
  \text{Negative predictive value (NPV)} = \frac{ \text{Number of true negatives} }{ \text{Number of predicted negatives} } \, ,
$$
The number of predicted positives in PPV is the sum of true positives and false positives, and the number of predicted negatives in NPV is the sum of true negatives and the false negatives. Considering PPV and NPV together gives a two-sided accuracy measures. For example, a classifier that predicts all observations as negative except for one correct positive observation achieves a PPV of $100\%$ but a small NPV\@. The combination of PPV and NPV gives the overall accuracy of the model.

In contrast, the area under the receiver operator characteristic (ROC) curve is a single measure that captures the effectiveness of a classifier. The ROC curve is a plot of the true positive rate against the false positive rate for different classification thresholds. The area under the curve (AUC) provides a measure of discrimination between positive and negative classes. The AUC can be interpreted as the probability that a positive observation is ranked higher than a negative observation. The AUC does not depend on the classification threshold as it is an aggregate measure. An AUC closer to 1 is reflective of a good model, while a random predictor will give an AUC closer to 0.5.

The graphs in Figures~\ref{fig:RealPPV},~\ref{fig:RealNPV} and~\ref{fig:RealROC} give the PPV, NPV and AUC values of the 3 classifiers grouped by classifier and fold. For PPV and NPV, we use a probability threshold of $0.5$, i.e. if the output probability is greater than 0.5, it is deemed class A, and class B otherwise.  Table~\ref{tab:AverageAccuracyReal} gives the average PPV, NPV and AUC values with their standard deviations over the $4$-folds for the real data stream.

% PPV - dev events 0.964285714	1	1	0.964285714
% PPV - dma 0.683333333	0.816071429	0.85625	0.6375
% PPV - glm 0.792857143	0.673809524	0.673809524	0.721428571

% NPV - dev events 0.930591631	0.947510823	0.936273449	0.902435065
% NPV - dma 0.978968254	1	0.986842105	0.992647059
% NPV - glm 0.953860029	0.965097403	0.959541847	0.965891053

% AUC - dev events 0.947438672	0.973755411	0.968136724	0.93336039
% AUC - dma 0.874206349	0.985603408	0.902065826	0.943148926
% AUC - glm 0.873358586	0.819453463	0.816675685	0.843659812

\begin{figure}[!h]
	\centering
	\subfloat[][]{
		\includegraphics[width=0.48\textwidth]{./Graphics/Real_World_PPV_1.pdf}
		\label{fig:PPV1} % width=0.48\textwidth, height = 0.48\textheight
	}%
	\subfloat[][]{
		\includegraphics[width=0.48\textwidth]{./Graphics/Real_World_PPV_2.pdf}
		\label{fig:PPV2}
	}
	\caption{PPV of the 3 classifiers with 4-fold cross validation grouped by classifier in Figure~\ref{fig:PPV1} and by fold in Figure~\ref{fig:PPV2}.}
	\label{fig:RealPPV}
\end{figure}

\begin{figure}[!h]
	\centering
	\subfloat[][]{
		\includegraphics[width=0.48\textwidth]{./Graphics/Real_World_NPV_1.pdf}
		\label{fig:NPV1} % width=0.48\textwidth, height = 0.48\textheight
	}%
	\subfloat[][]{
		\includegraphics[width=0.48\textwidth]{./Graphics/Real_World_NPV_2.pdf}
		\label{fig:NPV2}
	}
	\caption{NPV of the 3 classifiers with 4-fold cross validation grouped by classifier in Figure~\ref{fig:NPV1} and by fold in Figure~\ref{fig:NPV2}.}
	\label{fig:RealNPV}
\end{figure}

\begin{figure}[!h]
	\centering
	\subfloat[][]{
		\includegraphics[width=0.48\textwidth]{./Graphics/Real_World_ROC_1.pdf}
		\label{fig:ROC1} % width=0.48\textwidth, height = 0.48\textheight
	}%
	\subfloat[][]{
		\includegraphics[width=0.48\textwidth]{./Graphics/Real_World_ROC_2.pdf}
		\label{fig:ROC2}
	}
	\caption{AUC of the 3 classifiers with 4-fold cross validation grouped by classifier in Figure~\ref{fig:ROC1} and by fold in Figure~\ref{fig:ROC2}.}
	\label{fig:RealROC}
\end{figure}


\begin{table}[!h]
	\centering
	\begin{tabular}{llccccccccc}
		\toprule
		Accuracy Measure & Classifier  & \multicolumn{4}{c}{Mean} & & \multicolumn{4}{c}{Standard deviation}   \\
		\cmidrule{3-6} \cmidrule{8-11}
		& & $t_1$ &  $t_2$ &  $t_3$ &  $t_4$   & &   $t_1$ &  $t_2$ &  $t_3$ &  $t_4$ \\
		\midrule
		PPV              & Age-varying events classifier & 95    & 95   & 100  & 95   & & 8 & 8 & 0 & 8    \\
		                 & Cascaded state space models  & 74     & 69   & 68   & 68   & & 18 & 15 & 12& 23   \\
		                 & Logistic regression          & 91     & 75   & 75   & 70   & & 16 & 31 & 31 & 34     \\
		\hline
		NPV              & Age-varying events classifier & 91    & 92   & 93   & 92    & & 6 & 3 & 4 & 5   \\
		                 & Cascaded state space models  & 98     & 100  & 99   & 100   & & 2 & 0 & 1 & 0   \\
		                 & Logistic regression          & 95     & 95   & 95   & 92    & & 1 & 1 & 3 & 5 \\   \\
		\hline
		AUC              & Age-varying events classifier & 93    & 94   & 96   & 94   & & 5 & 5 & 2 & 5    \\
		                 & Cascaded state space models  & 94     & 97   & 94   & 97   & & 7 & 0 & 5& 1    \\
		                 & Logistic regression          & 93     & 85   & 85   & 81   & & 7 & 15 & 15 & 14     \\
		\bottomrule
	\end{tabular}
	 \caption{Mean and standard deviation of PPV, NPV and AUC $(\%)$ over $4$ folds.}\label{tab:AverageAccuracyReal}
\end{table}

From Table~\ref{tab:AverageAccuracyReal}, we see that the age-varying events classifier has the highest average PPV and the cascaded state space models have the highest average NPV for all event ages. Of the two classifiers, state space models are more conservative in predicting class A events compared with the age-varying events classifier. That is, if the state space models predict an event as belonging to class A, it is more likely that the actual event is of class A compared with the age-varying classifier. This is supported by the high mean NPV values and the associated low standard deviations of the state space models. On the other hand, the age-varying events classifier is much more likely to catch all class A events compared with the state space models as seen by the higher PPV values. Whether it is better to be conservative and miss some class A events while being very accurate in terms of the predicted class A events, or to identify all class A events while giving some false positives, depends on the application. Often the cost of  false positives and false negatives are not the same; for example consider a medical test for cancer. Thus, we see that our proposed two models have different strengths. In addition, both these models outperform the logistic regression classifier as seen by the AUC values.

\subsection{Nitrogen dioxide monitoring}\label{sec:NO2Exp}

The second application focuses on event extraction from Nitrogen Dioxide $(\text{NO}_2)$ data from NASA's NEO website \cite{OMINO2}. The Ozone Monitoring Instrument (OMI) \cite{levelt2006ozone} aboard the Aura satellite records a variety of air quality measures including $\text{NO}_2$ concentrations around the world. We use some of this data to demonstrate the event extraction and feature computation process for a 3-dimensional data stream.

We use OMI $\text{NO}_2$ monthly data from March to June in the years 2008 and 2018 for a comparison study. For each month the data comes in a matrix of $1799 \times 3600$ dimensions. For ease of calculation we reduce the dimension of this matrix to $180 \times 360 $, by computing the average of each $10 \times 10$ block of data in the original matrix. The OMI $\text{NO}_2$ data for two months (March 2008 and March 2018) are shown in Figure~\ref{fig:NO2March2008}.

\begin{figure}[!htb]
	\centering
	\subfloat[][]{
		\includegraphics[width=0.48\textwidth]{./Graphics/NO2_March_2008_With_Bndry.pdf}
		\label{fig:NO2March1} % width=0.48\textwidth, height = 0.48\textheight
	}%
	\subfloat[][]{
		\includegraphics[width=0.48\textwidth]{./Graphics/NO2_March_2018_With_Bndry.pdf}
		\label{fig:NO2March2}
	}
	\caption{$\text{NO}_2$ data for March 2008 (Figure~\ref{fig:NO2March1})  and March 2018 (Figure~\ref{fig:NO2March2}). }
	\label{fig:NO2March2008}
\end{figure}

Using Algorithm~\ref{algo:events_extraction}, we extract events from this data set, which are clusters of high $\text{NO}_2$ levels spanning space and time. Events are extracted from a 3-dimensional data stream of $4 \times 180 \times 360$ array, where each $180 \times 360$ matrix corresponds to the $\text{NO}_2$ levels of a given month. The parameters used in Algorithm~\ref{algo:events_extraction} are $\alpha = 0.97$, $\epsilon=2$ and $\text{minPts} = 20$. There are 23 events/clusters in the 2008 data and 13 events/clusters in 2018 data. Figure~\ref{fig:ClustersNO2March2008} shows these clusters for March 2008 and March 2018 (corresponding to the data in Figure~\ref{fig:NO2March2008}) with each cluster/event depicted by a single colour.

\begin{figure}[!hb]
	\centering
	\subfloat[][]{
		\includegraphics[width=0.48\textwidth]{./Graphics/Clusters_NO2_March_2008_With_Bndry.pdf}
		\label{fig:ClustersNO2March20081} % width=0.48\textwidth, height = 0.48\textheight
	}%
	\subfloat[][]{
		\includegraphics[width=0.48\textwidth]{./Graphics/Clusters_NO2_March_2018_With_Bndry.pdf}
		\label{fig:ClustersNO2March20082}
	}
	\caption{Events extracted from $\text{NO}_2$ data for March 2008 (Figure~\ref{fig:ClustersNO2March20081} ) and March 2018 (Figure~\ref{fig:ClustersNO2March20082}). Colours do not reflect $\text{NO}_2$ levels. Each event is depicted by a single colour.}
	\label{fig:ClustersNO2March2008}
\end{figure}

%\begin{figure}[!htb]
%	\centering
%	\subfloat[][]{
%		\includegraphics[width=0.45\textwidth]{./Graphics/Clusters_NO2_March_2018_No_Bndry.pdf}
%		\label{fig:ClustersNO2March20181} % width=0.48\textwidth, height = 0.48\textheight
%	}%
%	\subfloat[][]{
%		\includegraphics[width=0.55\textwidth]{./Graphics/Clusters_NO2_March_2018_With_Bndry.pdf}
%		\label{fig:ClustersNO2March20182}
%	}
%	\caption{Events extracted from $\text{NO}_2$ data for March 2018. Colours do not reflect $\text{NO}_2$ levels.}
%	\label{fig:ClustersNO2March2018}
%\end{figure}

\begin{figure}[!htb]
	\centering
 \subfloat[][]{
	\includegraphics[width=0.48\textwidth]{./Graphics/High_Event_2008.pdf}
  \label{fig:HighestEventNO2200820181}
  }
  \subfloat[][]{
  \includegraphics[width=0.48\textwidth]{./Graphics/High_Event_2018.pdf} % Highest_Event_NO2_2008_2018.pdf
  \label{fig:HighestEventNO2200820182}
 }
 \caption{The two events with the highest average $\text{NO}_2$ levels for March 2008 (Figure~\ref{fig:HighestEventNO2200820181}) and March 2018 (Figure~\ref{fig:HighestEventNO2200820182}).}
 \label{fig:HighestEventNO220082018}
\end{figure}

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.48\textwidth]{./Graphics/Highest_Event_NO2_2008_2018.pdf}
     \caption{The average $\text{NO}_2$ levels for the two events in Figure~\ref{fig:HighestEventNO220082018}.}
    \label{fig:HighestEventNO2200820183}
\end{figure}

\begin{figure}[!htb]
  \centering
 \subfloat[][]{
   \includegraphics[width=0.45\textwidth]{./Graphics//NO2_Level_Difference_2018_2008.pdf}
   \label{fig:DifferenceInNO2Levels1}
 }
 \subfloat[][]{
    \includegraphics[width=0.45\textwidth]{./Graphics/NO2_Level_Increase_2018_2008.pdf}
   \label{fig:DifferenceInNO2Levels2}
 }
  \caption{(a) Difference in average $\text{NO}_2$ levels (2018 -- 2008) for all matched events. (b) The events for which average $\text{NO}_2$ level difference is positive (average $\text{NO}_2$ levels have increased from 2008 to 2018).}
  \label{fig:DifferenceInNO2Levels}
\end{figure}

For each event we compute features 1--8 and 14 from the list of features in Section~\ref{sec:Featurelist}. These features are chosen for ease of computation. Specially as we are not focusing on event classification in this application, we do not compute the other features which involve fitting splines for 3-dimensional data.

We analyse the two events that had the highest average $\text{NO}_2$ levels for each of 2008 and 2018. The maps in Figure~\ref{fig:HighestEventNO220082018} show the spatial locations and the $\text{NO}_2$ levels of these two events for 2008 and 2018. As these two events are geographically at the same location, we compare the average  $\text{NO}_2$ levels for these two events for the months from March to June in 2008 with those of 2018. The graph in Figure~\ref{fig:HighestEventNO2200820183} shows that the average $\text{NO}_2$ levels have decreased in 2018 compared to 2008 for this geographical location.

Next we match the events in 2008 and 2018 by their spatial location. We do a simple analysis of these $\text{NO}_2$ events. We want to know if these $\text{NO}_2$ events have increased or decreased in severity during this 10 year time gap. For each of these matched events we find the average $\text{NO}_2$ level difference between 2018 and 2008. Figure~\ref{fig:DifferenceInNO2Levels} shows the graph of these differences. We see that 4 events have positive $\text{NO}_2$ differences for each month, i.e., the average $\text{NO}_2$ levels have increased from 2008 to 2018 for these 4 events. In addition, event 15 is different from other events. Figure~\ref{fig:PositiveDifferentEventsNO220082018} shows the locations of the increased $\text{NO}_2$ events in March 2018 along with the location of the outlying event 15.

\begin{figure}
	\centering
	\subfloat[][]{
		\includegraphics[width=0.45\textwidth]{./Graphics/Positive_NO2_Events_2018_2008.pdf}
		\label{fig:PositiveEventNO220082018}
	}%
	\subfloat[][]{
		\includegraphics[width=0.45\textwidth]{./Graphics/Different_Event_2018_2008.pdf}
		\label{fig:DifferentEventNO220082018}
	}
	\caption{The location in March 2018 of events with increased average $\text{NO}_2$ levels in Figure~\ref{fig:PositiveEventNO220082018}, and the outlying $\text{NO}_2$ event in Figure~\ref{fig:DifferentEventNO220082018}.}
	\label{fig:PositiveDifferentEventsNO220082018}
\end{figure}

While the increased industrialization in India and the ongoing war in the Middle-East  may be contributors, an investigation of reasons behind the increased $\text{NO}_2$ levels is beyond the scope of this study. Our aim is to demonstrate the applicability of our event extraction and event classification algorithms for different applications.

% =======================================================================
\section{Conclusions}\label{sec:Conclusions}
% =======================================================================

This paper has proposed a framework for event extraction and event classification in data streams with a focus on early event classification. We have introduced  time-varying coefficient models for event classification, and we have proposed two classifiers: a static and an updating classifier, both of which take developing event features/partial observations as input. We have tested our framework using 3 applications, one synthetic data application and two real data applications. We have shown that we obtain better accuracy results compared to logistic regression for these applications.

In addition, we have proposed a simple algorithm for event extraction, which can be used on two or three dimensional data streams. The applicability of our event extraction algorithm has been demonstrated using the $\text{NO}_2$ data from NASA's NEO website.

Future directions for this research include extending the age-varying events classifier to have automatically updating coefficients, and including a variety of event extraction processes.

% =======================================================================
\section{Supplementary material}
% =======================================================================
{\bf R-package eventstream:} This package contains Algorithm~\ref{algo:events_extraction} for event extraction, the age-varying event classifier for early event classification, functions for synthetic data generation, the fibre-optic data stream and $\text{NO}_2$ data for 2008 and 2018. It is available from Github at \url{https://github.com/sevvandi/eventstream}. \\
{\bf Scripts:} There are three files containing the R code used in Section~\ref{sec:Experiments}.  The file \url{Supp_Mat_1.R}, \url{Supp_Mat_2.R} and \url{Supp_Mat_3.R} contain the code applicable for synthetic data in Section~\ref{subsec:Synthetic}, fibre optic data in Section~\ref{sec:FibreOpticExperiment} and $\text{NO}_2$ data in Section~\ref{sec:NO2Exp} respectively. The file \url{Supp_Mat_4.R} contains the code used to produce some other graphs and images in the paper. \\
{\bf Other R-packages:} We have used the following R-packages either in this paper or within the package \textit{eventstream} : \textit{dma} \cite{dma}, \, \textit{abind} \cite{abind}, \, \textit{AtmRay} \cite{atmray}, \, \textit{pROC} \cite{proc}, \,  \textit{reshape2} \cite{reshape2}, \,  \textit{ggplot2} \cite{ggplot2}, \, \textit{raster} \cite{raster},  \,  \textit{maps} \cite{maps}, \,  \textit{tensorA} \cite{tensorA}, \,  \textit{glmnet} \cite{glmnet}, \,  \textit{dbscan} \cite{dbscan} and \textit{MASS} \cite{MASS}.

% =======================================================================
\section*{Acknowledgements}
% =======================================================================

Funding was provided by the Australian Research Council through the Linkage Project LP160101885. The first author would like to thank Dr Andres Ramirez Hassan for useful discussions.

\footnotesize
\bibliographystyle{IEEEtran} %Choose a bibliograhpic style
\bibliography{Master}

%1. A clustering approach for structural health monitoring on bridges \\
%2. Event Detection from Video Surveillance Data Based on Optical Flow Histogram and High-level Feature Extraction \\
% =======================================================================
\end{document}
% =======================================================================

%% Outline
%% 1. Introduction
%% 1.1		data streams - what is being done - literature
%% 1.2 		What they don't address
%% 1.3 What we do - a statistical approach for event classification
%% 2. Motivation & research problem
%% 2.1 		Real world example - early detection really important
%% 3 Notation - Our partial information model
%% 4. Partial observations classifiers
%%		- Explain the penalty and everything
%% 5. Cascaded Dynamic Linear Models for our work
%% 6. Datasets, Blob extraction, Features
%% 7 Results
%% 8. Future work and Conclusions
